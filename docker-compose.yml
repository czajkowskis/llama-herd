services:
  # Ollama AI Model Service
  ollama:
    image: ollama/ollama:latest
    container_name: llama-herd-ollama
    restart: unless-stopped
    ports:
      - "11435:11434"  # Use port 11435 on host to avoid conflict
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - llama-herd-network
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Backend FastAPI Service
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: llama-herd-backend
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./backend/data:/app/data
    networks:
      - llama-herd-network
    environment:
      # API Configuration
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_TITLE=LLaMa-Herd Backend
      - API_VERSION=1.0.0
      
      # CORS Configuration
      - CORS_ORIGINS=http://localhost:3000,http://localhost:80,http://localhost,http://frontend-prod:80
      - CORS_ALLOW_CREDENTIALS=true
      - CORS_ALLOW_METHODS=*
      - CORS_ALLOW_HEADERS=*
      
      # Ollama Configuration (using container name)
      - OLLAMA_BASE_URL=http://ollama:11434/v1
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_API_KEY=ollama
      - OLLAMA_TIMEOUT=300
      - OLLAMA_MODELS_DIR=/app/data/models
      
      # Storage Configuration
      - DATA_DIRECTORY=/app/data
      - EXPERIMENTS_DIRECTORY=experiments
      - CONVERSATIONS_DIRECTORY=conversations
      
      # Experiment Configuration
      - DEFAULT_MAX_ROUNDS=8
      - DEFAULT_TEMPERATURE=0.7
      - EXPERIMENT_TIMEOUT_SECONDS=3600
      - ITERATION_TIMEOUT_SECONDS=300
      
      # Pull Configuration
      - PULL_PROGRESS_THROTTLE_MS=500
      - PULL_PROGRESS_PERCENT_DELTA=2.0
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - dev
      - prod

  # Frontend React Service - Development
  frontend-dev:
    build:
      context: .
      dockerfile: Dockerfile
      target: dev
    container_name: llama-herd-frontend-dev
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - .:/app
      - /app/node_modules
    networks:
      - llama-herd-network
    environment:
      - REACT_APP_API_BASE_URL=http://localhost:8000
      - REACT_APP_OLLAMA_BASE_URL=http://ollama:11434
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
    depends_on:
      - backend
    profiles:
      - dev

  # Frontend React Service - Production
  frontend-prod:
    build:
      context: .
      dockerfile: Dockerfile
      target: prod
      args:
        REACT_APP_API_BASE_URL: 
        REACT_APP_OLLAMA_BASE_URL: http://ollama:11434
    container_name: llama-herd-frontend-prod
    restart: unless-stopped
    ports:
      - "80:80"
    networks:
      - llama-herd-network
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - prod

# Networks
networks:
  llama-herd-network:
    driver: bridge
    name: llama-herd-network

# Volumes
volumes:
  experiment-data:
    name: llama-herd-experiment-data
    driver: local
  ollama-models:
    name: llama-herd-ollama-models
    driver: local

