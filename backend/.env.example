# Backend Environment Configuration
# Copy this file to .env and adjust as needed for your local setup

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_TITLE=LLaMa-Herd Backend
API_VERSION=1.0.0

# CORS Configuration (for local frontend on port 3000)
CORS_ORIGINS=http://localhost:3000,http://localhost:3001
CORS_ALLOW_CREDENTIALS=true
CORS_ALLOW_METHODS=*
CORS_ALLOW_HEADERS=*

# Ollama Configuration (local Ollama installation)
# For Ollama running locally on standard port 11434
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_URL=http://localhost:11434
OLLAMA_API_KEY=ollama
OLLAMA_TIMEOUT=300
OLLAMA_MODELS_DIR=~/.ollama/models

# Storage Configuration (relative paths for local development)
DATA_DIRECTORY=./data
EXPERIMENTS_DIRECTORY=experiments
CONVERSATIONS_DIRECTORY=conversations

# Experiment Configuration
DEFAULT_MAX_ROUNDS=8
DEFAULT_TEMPERATURE=0.7
EXPERIMENT_TIMEOUT_SECONDS=3600
ITERATION_TIMEOUT_SECONDS=300

# Pull Configuration
PULL_PROGRESS_THROTTLE_MS=500
PULL_PROGRESS_PERCENT_DELTA=2.0
