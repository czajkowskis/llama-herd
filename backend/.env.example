# LLaMa-Herd Backend Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# API Configuration
# =============================================================================

# Host to bind the API server (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
API_HOST=0.0.0.0

# Port to bind the API server
API_PORT=8000

# API metadata
# API_TITLE="LLaMa-Herd Backend"
# API_VERSION=1.0.0

# =============================================================================
# CORS Configuration
# =============================================================================

# Allowed CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Allow credentials in CORS requests
CORS_ALLOW_CREDENTIALS=true

# Allowed HTTP methods for CORS (comma-separated, or * for all)
# CORS_ALLOW_METHODS=*

# Allowed HTTP headers for CORS (comma-separated, or * for all)
# CORS_ALLOW_HEADERS=*

# =============================================================================
# Ollama Configuration
# =============================================================================

# Base URL for Ollama API
# Default uses the proxy server on port 8080
OLLAMA_BASE_URL=http://localhost:8080/v1

# API key for Ollama (if authentication is required)
OLLAMA_API_KEY=ollama

# Timeout in seconds for Ollama API requests
OLLAMA_TIMEOUT=300

# =============================================================================
# Storage Configuration
# =============================================================================

# Root directory for data storage (relative to backend directory)
DATA_DIRECTORY=data

# Subdirectory for experiment data (relative to DATA_DIRECTORY)
EXPERIMENTS_DIRECTORY=experiments

# Subdirectory for conversation data (relative to DATA_DIRECTORY)
CONVERSATIONS_DIRECTORY=conversations

# =============================================================================
# Experiment Configuration
# =============================================================================

# Default maximum rounds for agent conversations
DEFAULT_MAX_ROUNDS=8

# Default temperature for LLM inference (0.0 to 1.0)
DEFAULT_TEMPERATURE=0.7

# =============================================================================
# Notes
# =============================================================================
#
# - All settings have defaults and are optional
# - Environment variables override .env file values
# - List values (like CORS_ORIGINS) should be comma-separated
# - Boolean values: true/false, yes/no, 1/0
# - Commented lines show default values
#
